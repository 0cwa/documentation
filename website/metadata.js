module.exports = {
  "event_types": [
    "log",
    "metric"
  ],
  "guides": {
    "advanced": {
      "children": {
      },
      "description": "Go beyond the basics, become a Qovery pro, and extract the full potential of Qovery.",
      "guides": [
        {
          "author_github": "https://github.com/a-rodin",
          "description": null,
          "id": "/advanced/custom-aggregations-with-lua",
          "last_modified_on": null,
          "path": "website/guides/advanced/custom-aggregations-with-lua.md",
          "series_position": null,
          "title": "Custom Aggregations with Lua"
        },
        {
          "author_github": "https://github.com/Jeffail",
          "description": null,
          "id": "/advanced/managing-complex-configs",
          "last_modified_on": null,
          "path": "website/guides/advanced/managing-complex-configs.md",
          "series_position": null,
          "title": "Managing Complex Configs"
        },
        {
          "author_github": "https://github.com/a-rodin",
          "description": null,
          "id": "/advanced/parsing-csv-logs-with-lua",
          "last_modified_on": null,
          "path": "website/guides/advanced/parsing-csv-logs-with-lua.md",
          "series_position": null,
          "title": "Parsing CSV logs with Lua"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/advanced/troubleshooting",
          "last_modified_on": null,
          "path": "website/guides/advanced/troubleshooting.md",
          "series_position": null,
          "title": "Troubleshooting"
        },
        {
          "author_github": "https://github.com/Jeffail",
          "description": null,
          "id": "/advanced/unit-testing",
          "last_modified_on": null,
          "path": "website/guides/advanced/unit-testing.md",
          "series_position": null,
          "title": "Unit Testing Your Configs"
        }
      ],
      "name": "advanced",
      "series": false,
      "title": "Advanced"
    },
    "getting-started": {
      "children": {
      },
      "description": "Take Qovery from zero to production in under 10 minutes.",
      "guides": [
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/getting-started/deploying",
          "last_modified_on": null,
          "path": "website/guides/getting-started/deploying.md",
          "series_position": null,
          "title": "Deploying Qovery"
        },
        {
          "author_github": "https://github.com/Jeffail",
          "description": null,
          "id": "/getting-started/your-first-pipeline",
          "last_modified_on": null,
          "path": "website/guides/getting-started/your-first-pipeline.md",
          "series_position": null,
          "title": "Hello World. Your First Qovery Pipeline."
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/getting-started/monitoring",
          "last_modified_on": null,
          "path": "website/guides/getting-started/monitoring.md",
          "series_position": null,
          "title": "Monitoring Qovery"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/getting-started/next-steps",
          "last_modified_on": null,
          "path": "website/guides/getting-started/next-steps.md",
          "series_position": null,
          "title": "Next Steps"
        },
        {
          "author_github": "https://github.com/Jeffail",
          "description": null,
          "id": "/getting-started/structuring",
          "last_modified_on": null,
          "path": "website/guides/getting-started/structuring.md",
          "series_position": null,
          "title": "Structuring Your Log Data"
        }
      ],
      "name": "getting-started",
      "series": true,
      "title": "Getting Started"
    },
    "integrate": {
      "children": {
      },
      "description": "Targeted guides for integrating platforms, data sources, and data destinations.",
      "guides": [
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker.md",
          "series_position": null,
          "title": "Collect logs from Docker and send them anywhere"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http.md",
          "series_position": null,
          "title": "Collect logs from HTTP and send them anywhere"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald.md",
          "series_position": null,
          "title": "Collect logs from Journald and send them anywhere"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka.md",
          "series_position": null,
          "title": "Collect logs from Kafka and send them anywhere"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin.md",
          "series_position": null,
          "title": "Collect logs from STDIN and send them anywhere"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec.md",
          "series_position": null,
          "title": "Collect logs from Splunk HEC and send them anywhere"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog.md",
          "series_position": null,
          "title": "Collect logs from Syslog and send them anywhere"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket.md",
          "series_position": null,
          "title": "Collect logs from a TCP, UDP, or UDS socket and send them anywhere"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file.md",
          "series_position": null,
          "title": "Collect logs from a file and send them anywhere"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/prometheus",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/prometheus.md",
          "series_position": null,
          "title": "Collect metrics from Prometheus and send them anywhere"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/statsd",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/statsd.md",
          "series_position": null,
          "title": "Collect metrics from Statsd and send them anywhere"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/aws_cloudwatch_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/aws_cloudwatch_logs.md",
          "series_position": null,
          "title": "Send logs from Docker to AWS Cloudwatch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/aws_kinesis_streams",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/aws_kinesis_streams.md",
          "series_position": null,
          "title": "Send logs from Docker to AWS Kinesis Data Streams"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/aws_kinesis_firehose",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/aws_kinesis_firehose.md",
          "series_position": null,
          "title": "Send logs from Docker to AWS Kinesis Firehose"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/aws_s3",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/aws_s3.md",
          "series_position": null,
          "title": "Send logs from Docker to AWS S3"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/pulsar",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/pulsar.md",
          "series_position": null,
          "title": "Send logs from Docker to Apache Pulsar"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/clickhouse",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/clickhouse.md",
          "series_position": null,
          "title": "Send logs from Docker to Clickhouse"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/datadog_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/datadog_logs.md",
          "series_position": null,
          "title": "Send logs from Docker to Datadog"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/elasticsearch",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/elasticsearch.md",
          "series_position": null,
          "title": "Send logs from Docker to Elasticsearch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/gcp_cloud_storage",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/gcp_cloud_storage.md",
          "series_position": null,
          "title": "Send logs from Docker to GCP Cloud Storage (GCS)"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/gcp_pubsub",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/gcp_pubsub.md",
          "series_position": null,
          "title": "Send logs from Docker to GCP PubSub"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/gcp_stackdriver_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/gcp_stackdriver_logs.md",
          "series_position": null,
          "title": "Send logs from Docker to GCP Stackdriver"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/honeycomb",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/honeycomb.md",
          "series_position": null,
          "title": "Send logs from Docker to Honeycomb"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/humio_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/humio_logs.md",
          "series_position": null,
          "title": "Send logs from Docker to Humio"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/kafka",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/kafka.md",
          "series_position": null,
          "title": "Send logs from Docker to Kafka"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/logdna",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/logdna.md",
          "series_position": null,
          "title": "Send logs from Docker to LogDNA"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/loki",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/loki.md",
          "series_position": null,
          "title": "Send logs from Docker to Loki"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/new_relic_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/new_relic_logs.md",
          "series_position": null,
          "title": "Send logs from Docker to New Relic"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/papertrail",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/papertrail.md",
          "series_position": null,
          "title": "Send logs from Docker to Papertrail"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/sematext_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/sematext_logs.md",
          "series_position": null,
          "title": "Send logs from Docker to Sematext"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/splunk_hec",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/splunk_hec.md",
          "series_position": null,
          "title": "Send logs from Docker to a Splunk HEC"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/socket",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/socket.md",
          "series_position": null,
          "title": "Send logs from Docker to a TCP, UDP, or UDS socket"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/file",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/file.md",
          "series_position": null,
          "title": "Send logs from Docker to a file"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/platforms/docker/http",
          "last_modified_on": null,
          "path": "website/guides/integrate/platforms/docker/http.md",
          "series_position": null,
          "title": "Send logs from Docker to an HTTP endpoint"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/aws_cloudwatch_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/aws_cloudwatch_logs.md",
          "series_position": null,
          "title": "Send logs from HTTP to AWS Cloudwatch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/aws_kinesis_streams",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/aws_kinesis_streams.md",
          "series_position": null,
          "title": "Send logs from HTTP to AWS Kinesis Data Streams"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/aws_kinesis_firehose",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/aws_kinesis_firehose.md",
          "series_position": null,
          "title": "Send logs from HTTP to AWS Kinesis Firehose"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/aws_s3",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/aws_s3.md",
          "series_position": null,
          "title": "Send logs from HTTP to AWS S3"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/pulsar",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/pulsar.md",
          "series_position": null,
          "title": "Send logs from HTTP to Apache Pulsar"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/clickhouse",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/clickhouse.md",
          "series_position": null,
          "title": "Send logs from HTTP to Clickhouse"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/datadog_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/datadog_logs.md",
          "series_position": null,
          "title": "Send logs from HTTP to Datadog"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/elasticsearch",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/elasticsearch.md",
          "series_position": null,
          "title": "Send logs from HTTP to Elasticsearch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/gcp_cloud_storage",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/gcp_cloud_storage.md",
          "series_position": null,
          "title": "Send logs from HTTP to GCP Cloud Storage (GCS)"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/gcp_pubsub",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/gcp_pubsub.md",
          "series_position": null,
          "title": "Send logs from HTTP to GCP PubSub"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/gcp_stackdriver_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/gcp_stackdriver_logs.md",
          "series_position": null,
          "title": "Send logs from HTTP to GCP Stackdriver"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/honeycomb",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/honeycomb.md",
          "series_position": null,
          "title": "Send logs from HTTP to Honeycomb"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/humio_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/humio_logs.md",
          "series_position": null,
          "title": "Send logs from HTTP to Humio"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/kafka",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/kafka.md",
          "series_position": null,
          "title": "Send logs from HTTP to Kafka"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/logdna",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/logdna.md",
          "series_position": null,
          "title": "Send logs from HTTP to LogDNA"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/loki",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/loki.md",
          "series_position": null,
          "title": "Send logs from HTTP to Loki"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/new_relic_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/new_relic_logs.md",
          "series_position": null,
          "title": "Send logs from HTTP to New Relic"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/papertrail",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/papertrail.md",
          "series_position": null,
          "title": "Send logs from HTTP to Papertrail"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/sematext_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/sematext_logs.md",
          "series_position": null,
          "title": "Send logs from HTTP to Sematext"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/splunk_hec",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/splunk_hec.md",
          "series_position": null,
          "title": "Send logs from HTTP to a Splunk HEC"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/socket",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/socket.md",
          "series_position": null,
          "title": "Send logs from HTTP to a TCP, UDP, or UDS socket"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/file",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/file.md",
          "series_position": null,
          "title": "Send logs from HTTP to a file"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/http",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/http.md",
          "series_position": null,
          "title": "Send logs from HTTP to an HTTP endpoint"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/http/qovery",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/http/qovery.md",
          "series_position": null,
          "title": "Send logs from HTTP to another Qovery instance"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/aws_cloudwatch_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/aws_cloudwatch_logs.md",
          "series_position": null,
          "title": "Send logs from Journald to AWS Cloudwatch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/aws_kinesis_streams",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/aws_kinesis_streams.md",
          "series_position": null,
          "title": "Send logs from Journald to AWS Kinesis Data Streams"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/aws_kinesis_firehose",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/aws_kinesis_firehose.md",
          "series_position": null,
          "title": "Send logs from Journald to AWS Kinesis Firehose"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/aws_s3",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/aws_s3.md",
          "series_position": null,
          "title": "Send logs from Journald to AWS S3"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/pulsar",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/pulsar.md",
          "series_position": null,
          "title": "Send logs from Journald to Apache Pulsar"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/clickhouse",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/clickhouse.md",
          "series_position": null,
          "title": "Send logs from Journald to Clickhouse"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/datadog_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/datadog_logs.md",
          "series_position": null,
          "title": "Send logs from Journald to Datadog"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/elasticsearch",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/elasticsearch.md",
          "series_position": null,
          "title": "Send logs from Journald to Elasticsearch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/gcp_cloud_storage",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/gcp_cloud_storage.md",
          "series_position": null,
          "title": "Send logs from Journald to GCP Cloud Storage (GCS)"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/gcp_pubsub",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/gcp_pubsub.md",
          "series_position": null,
          "title": "Send logs from Journald to GCP PubSub"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/gcp_stackdriver_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/gcp_stackdriver_logs.md",
          "series_position": null,
          "title": "Send logs from Journald to GCP Stackdriver"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/honeycomb",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/honeycomb.md",
          "series_position": null,
          "title": "Send logs from Journald to Honeycomb"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/humio_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/humio_logs.md",
          "series_position": null,
          "title": "Send logs from Journald to Humio"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/kafka",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/kafka.md",
          "series_position": null,
          "title": "Send logs from Journald to Kafka"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/logdna",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/logdna.md",
          "series_position": null,
          "title": "Send logs from Journald to LogDNA"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/loki",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/loki.md",
          "series_position": null,
          "title": "Send logs from Journald to Loki"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/new_relic_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/new_relic_logs.md",
          "series_position": null,
          "title": "Send logs from Journald to New Relic"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/papertrail",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/papertrail.md",
          "series_position": null,
          "title": "Send logs from Journald to Papertrail"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/sematext_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/sematext_logs.md",
          "series_position": null,
          "title": "Send logs from Journald to Sematext"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/splunk_hec",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/splunk_hec.md",
          "series_position": null,
          "title": "Send logs from Journald to a Splunk HEC"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/socket",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/socket.md",
          "series_position": null,
          "title": "Send logs from Journald to a TCP, UDP, or UDS socket"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/file",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/file.md",
          "series_position": null,
          "title": "Send logs from Journald to a file"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/http",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/http.md",
          "series_position": null,
          "title": "Send logs from Journald to an HTTP endpoint"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/journald/qovery",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/journald/qovery.md",
          "series_position": null,
          "title": "Send logs from Journald to another Qovery instance"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/aws_cloudwatch_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/aws_cloudwatch_logs.md",
          "series_position": null,
          "title": "Send logs from Kafka to AWS Cloudwatch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/aws_kinesis_streams",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/aws_kinesis_streams.md",
          "series_position": null,
          "title": "Send logs from Kafka to AWS Kinesis Data Streams"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/aws_kinesis_firehose",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/aws_kinesis_firehose.md",
          "series_position": null,
          "title": "Send logs from Kafka to AWS Kinesis Firehose"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/aws_s3",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/aws_s3.md",
          "series_position": null,
          "title": "Send logs from Kafka to AWS S3"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/pulsar",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/pulsar.md",
          "series_position": null,
          "title": "Send logs from Kafka to Apache Pulsar"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/clickhouse",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/clickhouse.md",
          "series_position": null,
          "title": "Send logs from Kafka to Clickhouse"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/datadog_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/datadog_logs.md",
          "series_position": null,
          "title": "Send logs from Kafka to Datadog"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/elasticsearch",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/elasticsearch.md",
          "series_position": null,
          "title": "Send logs from Kafka to Elasticsearch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/gcp_cloud_storage",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/gcp_cloud_storage.md",
          "series_position": null,
          "title": "Send logs from Kafka to GCP Cloud Storage (GCS)"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/gcp_pubsub",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/gcp_pubsub.md",
          "series_position": null,
          "title": "Send logs from Kafka to GCP PubSub"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/gcp_stackdriver_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/gcp_stackdriver_logs.md",
          "series_position": null,
          "title": "Send logs from Kafka to GCP Stackdriver"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/honeycomb",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/honeycomb.md",
          "series_position": null,
          "title": "Send logs from Kafka to Honeycomb"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/humio_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/humio_logs.md",
          "series_position": null,
          "title": "Send logs from Kafka to Humio"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/kafka",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/kafka.md",
          "series_position": null,
          "title": "Send logs from Kafka to Kafka"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/logdna",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/logdna.md",
          "series_position": null,
          "title": "Send logs from Kafka to LogDNA"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/loki",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/loki.md",
          "series_position": null,
          "title": "Send logs from Kafka to Loki"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/new_relic_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/new_relic_logs.md",
          "series_position": null,
          "title": "Send logs from Kafka to New Relic"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/papertrail",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/papertrail.md",
          "series_position": null,
          "title": "Send logs from Kafka to Papertrail"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/sematext_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/sematext_logs.md",
          "series_position": null,
          "title": "Send logs from Kafka to Sematext"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/splunk_hec",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/splunk_hec.md",
          "series_position": null,
          "title": "Send logs from Kafka to a Splunk HEC"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/socket",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/socket.md",
          "series_position": null,
          "title": "Send logs from Kafka to a TCP, UDP, or UDS socket"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/file",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/file.md",
          "series_position": null,
          "title": "Send logs from Kafka to a file"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/http",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/http.md",
          "series_position": null,
          "title": "Send logs from Kafka to an HTTP endpoint"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/kafka/qovery",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/kafka/qovery.md",
          "series_position": null,
          "title": "Send logs from Kafka to another Qovery instance"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/aws_cloudwatch_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/aws_cloudwatch_logs.md",
          "series_position": null,
          "title": "Send logs from STDIN to AWS Cloudwatch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/aws_kinesis_streams",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/aws_kinesis_streams.md",
          "series_position": null,
          "title": "Send logs from STDIN to AWS Kinesis Data Streams"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/aws_kinesis_firehose",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/aws_kinesis_firehose.md",
          "series_position": null,
          "title": "Send logs from STDIN to AWS Kinesis Firehose"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/aws_s3",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/aws_s3.md",
          "series_position": null,
          "title": "Send logs from STDIN to AWS S3"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/pulsar",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/pulsar.md",
          "series_position": null,
          "title": "Send logs from STDIN to Apache Pulsar"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/clickhouse",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/clickhouse.md",
          "series_position": null,
          "title": "Send logs from STDIN to Clickhouse"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/datadog_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/datadog_logs.md",
          "series_position": null,
          "title": "Send logs from STDIN to Datadog"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/elasticsearch",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/elasticsearch.md",
          "series_position": null,
          "title": "Send logs from STDIN to Elasticsearch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/gcp_cloud_storage",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/gcp_cloud_storage.md",
          "series_position": null,
          "title": "Send logs from STDIN to GCP Cloud Storage (GCS)"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/gcp_pubsub",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/gcp_pubsub.md",
          "series_position": null,
          "title": "Send logs from STDIN to GCP PubSub"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/gcp_stackdriver_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/gcp_stackdriver_logs.md",
          "series_position": null,
          "title": "Send logs from STDIN to GCP Stackdriver"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/honeycomb",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/honeycomb.md",
          "series_position": null,
          "title": "Send logs from STDIN to Honeycomb"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/humio_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/humio_logs.md",
          "series_position": null,
          "title": "Send logs from STDIN to Humio"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/kafka",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/kafka.md",
          "series_position": null,
          "title": "Send logs from STDIN to Kafka"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/logdna",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/logdna.md",
          "series_position": null,
          "title": "Send logs from STDIN to LogDNA"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/loki",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/loki.md",
          "series_position": null,
          "title": "Send logs from STDIN to Loki"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/new_relic_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/new_relic_logs.md",
          "series_position": null,
          "title": "Send logs from STDIN to New Relic"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/papertrail",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/papertrail.md",
          "series_position": null,
          "title": "Send logs from STDIN to Papertrail"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/sematext_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/sematext_logs.md",
          "series_position": null,
          "title": "Send logs from STDIN to Sematext"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/splunk_hec",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/splunk_hec.md",
          "series_position": null,
          "title": "Send logs from STDIN to a Splunk HEC"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/socket",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/socket.md",
          "series_position": null,
          "title": "Send logs from STDIN to a TCP, UDP, or UDS socket"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/file",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/file.md",
          "series_position": null,
          "title": "Send logs from STDIN to a file"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/http",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/http.md",
          "series_position": null,
          "title": "Send logs from STDIN to an HTTP endpoint"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/stdin/qovery",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/stdin/qovery.md",
          "series_position": null,
          "title": "Send logs from STDIN to another Qovery instance"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/aws_cloudwatch_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/aws_cloudwatch_logs.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to AWS Cloudwatch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/aws_kinesis_streams",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/aws_kinesis_streams.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to AWS Kinesis Data Streams"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/aws_kinesis_firehose",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/aws_kinesis_firehose.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to AWS Kinesis Firehose"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/aws_s3",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/aws_s3.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to AWS S3"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/pulsar",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/pulsar.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to Apache Pulsar"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/clickhouse",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/clickhouse.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to Clickhouse"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/datadog_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/datadog_logs.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to Datadog"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/elasticsearch",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/elasticsearch.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to Elasticsearch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/gcp_cloud_storage",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/gcp_cloud_storage.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to GCP Cloud Storage (GCS)"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/gcp_pubsub",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/gcp_pubsub.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to GCP PubSub"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/gcp_stackdriver_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/gcp_stackdriver_logs.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to GCP Stackdriver"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/honeycomb",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/honeycomb.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to Honeycomb"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/humio_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/humio_logs.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to Humio"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/kafka",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/kafka.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to Kafka"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/logdna",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/logdna.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to LogDNA"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/loki",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/loki.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to Loki"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/new_relic_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/new_relic_logs.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to New Relic"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/papertrail",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/papertrail.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to Papertrail"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/sematext_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/sematext_logs.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to Sematext"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/splunk_hec",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/splunk_hec.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to a Splunk HEC"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/socket",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/socket.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to a TCP, UDP, or UDS socket"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/file",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/file.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to a file"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/http",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/http.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to an HTTP endpoint"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/splunk_hec/qovery",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/splunk_hec/qovery.md",
          "series_position": null,
          "title": "Send logs from Splunk HEC to another Qovery instance"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/aws_cloudwatch_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/aws_cloudwatch_logs.md",
          "series_position": null,
          "title": "Send logs from Syslog to AWS Cloudwatch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/aws_kinesis_streams",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/aws_kinesis_streams.md",
          "series_position": null,
          "title": "Send logs from Syslog to AWS Kinesis Data Streams"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/aws_kinesis_firehose",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/aws_kinesis_firehose.md",
          "series_position": null,
          "title": "Send logs from Syslog to AWS Kinesis Firehose"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/aws_s3",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/aws_s3.md",
          "series_position": null,
          "title": "Send logs from Syslog to AWS S3"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/pulsar",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/pulsar.md",
          "series_position": null,
          "title": "Send logs from Syslog to Apache Pulsar"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/clickhouse",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/clickhouse.md",
          "series_position": null,
          "title": "Send logs from Syslog to Clickhouse"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/datadog_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/datadog_logs.md",
          "series_position": null,
          "title": "Send logs from Syslog to Datadog"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/elasticsearch",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/elasticsearch.md",
          "series_position": null,
          "title": "Send logs from Syslog to Elasticsearch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/gcp_cloud_storage",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/gcp_cloud_storage.md",
          "series_position": null,
          "title": "Send logs from Syslog to GCP Cloud Storage (GCS)"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/gcp_pubsub",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/gcp_pubsub.md",
          "series_position": null,
          "title": "Send logs from Syslog to GCP PubSub"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/gcp_stackdriver_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/gcp_stackdriver_logs.md",
          "series_position": null,
          "title": "Send logs from Syslog to GCP Stackdriver"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/honeycomb",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/honeycomb.md",
          "series_position": null,
          "title": "Send logs from Syslog to Honeycomb"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/humio_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/humio_logs.md",
          "series_position": null,
          "title": "Send logs from Syslog to Humio"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/kafka",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/kafka.md",
          "series_position": null,
          "title": "Send logs from Syslog to Kafka"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/logdna",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/logdna.md",
          "series_position": null,
          "title": "Send logs from Syslog to LogDNA"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/loki",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/loki.md",
          "series_position": null,
          "title": "Send logs from Syslog to Loki"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/new_relic_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/new_relic_logs.md",
          "series_position": null,
          "title": "Send logs from Syslog to New Relic"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/papertrail",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/papertrail.md",
          "series_position": null,
          "title": "Send logs from Syslog to Papertrail"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/sematext_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/sematext_logs.md",
          "series_position": null,
          "title": "Send logs from Syslog to Sematext"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/splunk_hec",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/splunk_hec.md",
          "series_position": null,
          "title": "Send logs from Syslog to a Splunk HEC"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/socket",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/socket.md",
          "series_position": null,
          "title": "Send logs from Syslog to a TCP, UDP, or UDS socket"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/file",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/file.md",
          "series_position": null,
          "title": "Send logs from Syslog to a file"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/http",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/http.md",
          "series_position": null,
          "title": "Send logs from Syslog to an HTTP endpoint"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/syslog/qovery",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/syslog/qovery.md",
          "series_position": null,
          "title": "Send logs from Syslog to another Qovery instance"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/aws_cloudwatch_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/aws_cloudwatch_logs.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to AWS Cloudwatch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/aws_kinesis_streams",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/aws_kinesis_streams.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to AWS Kinesis Data Streams"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/aws_kinesis_firehose",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/aws_kinesis_firehose.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to AWS Kinesis Firehose"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/aws_s3",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/aws_s3.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to AWS S3"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/pulsar",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/pulsar.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to Apache Pulsar"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/clickhouse",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/clickhouse.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to Clickhouse"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/datadog_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/datadog_logs.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to Datadog"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/elasticsearch",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/elasticsearch.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to Elasticsearch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/gcp_cloud_storage",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/gcp_cloud_storage.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to GCP Cloud Storage (GCS)"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/gcp_pubsub",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/gcp_pubsub.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to GCP PubSub"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/gcp_stackdriver_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/gcp_stackdriver_logs.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to GCP Stackdriver"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/honeycomb",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/honeycomb.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to Honeycomb"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/humio_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/humio_logs.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to Humio"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/kafka",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/kafka.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to Kafka"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/logdna",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/logdna.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to LogDNA"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/loki",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/loki.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to Loki"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/new_relic_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/new_relic_logs.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to New Relic"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/papertrail",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/papertrail.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to Papertrail"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/sematext_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/sematext_logs.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to Sematext"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/splunk_hec",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/splunk_hec.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to a Splunk HEC"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/socket",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/socket.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to a TCP, UDP, or UDS socket"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/file",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/file.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to a file"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/http",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/http.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to an HTTP endpoint"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/socket/qovery",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/socket/qovery.md",
          "series_position": null,
          "title": "Send logs from a TCP, UDP, or UDS socket to another Qovery instance"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/aws_cloudwatch_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/aws_cloudwatch_logs.md",
          "series_position": null,
          "title": "Send logs from a file to AWS Cloudwatch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/aws_kinesis_streams",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/aws_kinesis_streams.md",
          "series_position": null,
          "title": "Send logs from a file to AWS Kinesis Data Streams"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/aws_kinesis_firehose",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/aws_kinesis_firehose.md",
          "series_position": null,
          "title": "Send logs from a file to AWS Kinesis Firehose"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/aws_s3",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/aws_s3.md",
          "series_position": null,
          "title": "Send logs from a file to AWS S3"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/pulsar",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/pulsar.md",
          "series_position": null,
          "title": "Send logs from a file to Apache Pulsar"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/clickhouse",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/clickhouse.md",
          "series_position": null,
          "title": "Send logs from a file to Clickhouse"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/datadog_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/datadog_logs.md",
          "series_position": null,
          "title": "Send logs from a file to Datadog"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/elasticsearch",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/elasticsearch.md",
          "series_position": null,
          "title": "Send logs from a file to Elasticsearch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/gcp_cloud_storage",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/gcp_cloud_storage.md",
          "series_position": null,
          "title": "Send logs from a file to GCP Cloud Storage (GCS)"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/gcp_pubsub",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/gcp_pubsub.md",
          "series_position": null,
          "title": "Send logs from a file to GCP PubSub"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/gcp_stackdriver_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/gcp_stackdriver_logs.md",
          "series_position": null,
          "title": "Send logs from a file to GCP Stackdriver"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/honeycomb",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/honeycomb.md",
          "series_position": null,
          "title": "Send logs from a file to Honeycomb"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/humio_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/humio_logs.md",
          "series_position": null,
          "title": "Send logs from a file to Humio"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/kafka",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/kafka.md",
          "series_position": null,
          "title": "Send logs from a file to Kafka"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/logdna",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/logdna.md",
          "series_position": null,
          "title": "Send logs from a file to LogDNA"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/loki",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/loki.md",
          "series_position": null,
          "title": "Send logs from a file to Loki"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/new_relic_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/new_relic_logs.md",
          "series_position": null,
          "title": "Send logs from a file to New Relic"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/papertrail",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/papertrail.md",
          "series_position": null,
          "title": "Send logs from a file to Papertrail"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/sematext_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/sematext_logs.md",
          "series_position": null,
          "title": "Send logs from a file to Sematext"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/splunk_hec",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/splunk_hec.md",
          "series_position": null,
          "title": "Send logs from a file to a Splunk HEC"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/socket",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/socket.md",
          "series_position": null,
          "title": "Send logs from a file to a TCP, UDP, or UDS socket"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/file",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/file.md",
          "series_position": null,
          "title": "Send logs from a file to a file"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/http",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/http.md",
          "series_position": null,
          "title": "Send logs from a file to an HTTP endpoint"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/file/qovery",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/file/qovery.md",
          "series_position": null,
          "title": "Send logs from a file to another Qovery instance"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/aws_cloudwatch_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/aws_cloudwatch_logs.md",
          "series_position": null,
          "title": "Send logs to AWS Cloudwatch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/aws_kinesis_streams",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/aws_kinesis_streams.md",
          "series_position": null,
          "title": "Send logs to AWS Kinesis Data Streams"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/aws_kinesis_firehose",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/aws_kinesis_firehose.md",
          "series_position": null,
          "title": "Send logs to AWS Kinesis Firehose"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/aws_s3",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/aws_s3.md",
          "series_position": null,
          "title": "Send logs to AWS S3"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/pulsar",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/pulsar.md",
          "series_position": null,
          "title": "Send logs to Apache Pulsar"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/clickhouse",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/clickhouse.md",
          "series_position": null,
          "title": "Send logs to Clickhouse"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/datadog_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/datadog_logs.md",
          "series_position": null,
          "title": "Send logs to Datadog"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/elasticsearch",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/elasticsearch.md",
          "series_position": null,
          "title": "Send logs to Elasticsearch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/gcp_cloud_storage",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/gcp_cloud_storage.md",
          "series_position": null,
          "title": "Send logs to GCP Cloud Storage (GCS)"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/gcp_pubsub",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/gcp_pubsub.md",
          "series_position": null,
          "title": "Send logs to GCP PubSub"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/gcp_stackdriver_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/gcp_stackdriver_logs.md",
          "series_position": null,
          "title": "Send logs to GCP Stackdriver"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/honeycomb",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/honeycomb.md",
          "series_position": null,
          "title": "Send logs to Honeycomb"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/humio_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/humio_logs.md",
          "series_position": null,
          "title": "Send logs to Humio"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/kafka",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/kafka.md",
          "series_position": null,
          "title": "Send logs to Kafka"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/logdna",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/logdna.md",
          "series_position": null,
          "title": "Send logs to LogDNA"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/loki",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/loki.md",
          "series_position": null,
          "title": "Send logs to Loki"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/new_relic_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/new_relic_logs.md",
          "series_position": null,
          "title": "Send logs to New Relic"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/papertrail",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/papertrail.md",
          "series_position": null,
          "title": "Send logs to Papertrail"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/sematext_logs",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/sematext_logs.md",
          "series_position": null,
          "title": "Send logs to Sematext"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/splunk_hec",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/splunk_hec.md",
          "series_position": null,
          "title": "Send logs to a Splunk HEC"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/socket",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/socket.md",
          "series_position": null,
          "title": "Send logs to a TCP, UDP, or UDS socket"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/file",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/file.md",
          "series_position": null,
          "title": "Send logs to a file"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/http",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/http.md",
          "series_position": null,
          "title": "Send logs to an HTTP endpoint"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/prometheus/aws_cloudwatch_metrics",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/prometheus/aws_cloudwatch_metrics.md",
          "series_position": null,
          "title": "Send metrics from Prometheus to AWS Cloudwatch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/prometheus/datadog_metrics",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/prometheus/datadog_metrics.md",
          "series_position": null,
          "title": "Send metrics from Prometheus to Datadog"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/prometheus/influxdb_metrics",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/prometheus/influxdb_metrics.md",
          "series_position": null,
          "title": "Send metrics from Prometheus to InfluxDB"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/prometheus/prometheus",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/prometheus/prometheus.md",
          "series_position": null,
          "title": "Send metrics from Prometheus to Prometheus"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/prometheus/statsd",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/prometheus/statsd.md",
          "series_position": null,
          "title": "Send metrics from Prometheus to Statsd"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/statsd/aws_cloudwatch_metrics",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/statsd/aws_cloudwatch_metrics.md",
          "series_position": null,
          "title": "Send metrics from Statsd to AWS Cloudwatch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/statsd/datadog_metrics",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/statsd/datadog_metrics.md",
          "series_position": null,
          "title": "Send metrics from Statsd to Datadog"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/statsd/influxdb_metrics",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/statsd/influxdb_metrics.md",
          "series_position": null,
          "title": "Send metrics from Statsd to InfluxDB"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/statsd/prometheus",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/statsd/prometheus.md",
          "series_position": null,
          "title": "Send metrics from Statsd to Prometheus"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sources/statsd/statsd",
          "last_modified_on": null,
          "path": "website/guides/integrate/sources/statsd/statsd.md",
          "series_position": null,
          "title": "Send metrics from Statsd to Statsd"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/aws_cloudwatch_metrics",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/aws_cloudwatch_metrics.md",
          "series_position": null,
          "title": "Send metrics to AWS Cloudwatch"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/datadog_metrics",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/datadog_metrics.md",
          "series_position": null,
          "title": "Send metrics to Datadog"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/influxdb_metrics",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/influxdb_metrics.md",
          "series_position": null,
          "title": "Send metrics to InfluxDB"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/prometheus",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/prometheus.md",
          "series_position": null,
          "title": "Send metrics to Prometheus"
        },
        {
          "author_github": "https://github.com/binarylogic",
          "description": null,
          "id": "/integrate/sinks/statsd",
          "last_modified_on": null,
          "path": "website/guides/integrate/sinks/statsd.md",
          "series_position": null,
          "title": "Send metrics to Statsd"
        }
      ],
      "name": "integrate",
      "series": false,
      "title": "Integrate"
    }
  },
  "installation": {
    "operating_systems": {
      "amazon-linux": {
        "title": "Amazon Linux",
        "interfaces": [
          "rpm",
          "qovery-cli",
          "docker-cli",
          "docker-compose"
        ],
        "os": "Linux",
        "strategies": [
          {
            "name": "daemon",
            "source": "journald"
          }
        ],
        "name": "amazon-linux"
      },
      "centos": {
        "title": "CentOS",
        "interfaces": [
          "rpm",
          "qovery-cli",
          "docker-cli",
          "docker-compose"
        ],
        "os": "Linux",
        "strategies": [
          {
            "name": "daemon",
            "source": "journald"
          }
        ],
        "name": "centos"
      },
      "debian": {
        "title": "Debian",
        "interfaces": [
          "dpkg",
          "qovery-cli",
          "docker-cli",
          "docker-compose"
        ],
        "os": "Linux",
        "strategies": [
          {
            "name": "daemon",
            "source": "journald"
          }
        ],
        "name": "debian"
      },
      "macos": {
        "title": "MacOS",
        "interfaces": [
          "homebrew",
          "qovery-cli",
          "docker-cli",
          "docker-compose"
        ],
        "os": "Linux",
        "strategies": [
          {
            "name": "daemon",
            "source": "file"
          }
        ],
        "name": "macos"
      },
      "nixos": {
        "title": "NixOS",
        "interfaces": [
          "nix",
          "qovery-cli",
          "docker-cli",
          "docker-compose"
        ],
        "os": "Linux",
        "strategies": [
          {
            "name": "daemon",
            "source": "journald"
          }
        ],
        "name": "nixos"
      },
      "raspbian": {
        "title": "Raspbian",
        "interfaces": [
          "qovery-cli",
          "docker-cli",
          "docker-compose"
        ],
        "os": "Linux",
        "strategies": [
          {
            "name": "daemon",
            "source": "journald"
          }
        ],
        "name": "raspbian"
      },
      "rhel": {
        "title": "RHEL",
        "interfaces": [
          "rpm",
          "qovery-cli",
          "docker-cli",
          "docker-compose"
        ],
        "os": "Linux",
        "strategies": [
          {
            "name": "daemon",
            "source": "journald"
          }
        ],
        "name": "rhel"
      },
      "ubuntu": {
        "title": "Ubuntu",
        "interfaces": [
          "dpkg",
          "qovery-cli",
          "docker-cli",
          "docker-compose"
        ],
        "os": "Linux",
        "strategies": [
          {
            "name": "daemon",
            "source": "journald"
          }
        ],
        "name": "ubuntu"
      },
      "windows": {
        "title": "Windows",
        "interfaces": [
          "msi",
          "qovery-cli",
          "docker-cli",
          "docker-compose"
        ],
        "os": "Windows",
        "strategies": [
          {
            "name": "daemon",
            "source": "file"
          }
        ],
        "name": "windows"
      }
    },
    "package_managers": {
      "dpkg": {
        "title": "DPKG",
        "archs": [
          "x86_64",
          "ARM64",
          "ARMv7"
        ],
        "interfaces": [
          "dpkg"
        ],
        "strategies": [
          {
            "name": "daemon",
            "source": "journald"
          }
        ],
        "name": "dpkg"
      },
      "homebrew": {
        "title": "Homebrew",
        "archs": [
          "x86_64"
        ],
        "interfaces": [
          "homebrew"
        ],
        "strategies": [
          {
            "name": "daemon",
            "source": "file"
          }
        ],
        "name": "homebrew"
      },
      "msi": {
        "title": "MSI",
        "interfaces": [
          "msi"
        ],
        "archs": [
          "x86_64"
        ],
        "strategies": [
          {
            "name": "daemon",
            "source": "file"
          }
        ],
        "name": "msi"
      },
      "nix": {
        "title": "Nix",
        "interfaces": [
          "nix"
        ],
        "archs": [
          "x86_64"
        ],
        "strategies": [
          {
            "name": "daemon",
            "source": "journald"
          }
        ],
        "name": "nix"
      },
      "rpm": {
        "title": "RPM",
        "interfaces": [
          "rpm"
        ],
        "archs": [
          "x86_64"
        ],
        "strategies": [
          {
            "name": "daemon",
            "source": "journald"
          }
        ],
        "name": "rpm"
      }
    },
    "platforms": {
      "docker": {
        "archs": [
          "x86_64",
          "ARM64",
          "ARMv7"
        ],
        "description": "[Docker][urls.docker] is an open platform for developing, shipping, and running\napplications and services. Docker enables you to separate your services from your infrastructure so you can ship quickly. With Docker, you can manage your infrastructure in the same ways you manage your services. By taking advantage of Docker’s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.",
        "interfaces": [
          "docker-cli",
          "docker-compose"
        ],
        "logo_path": "/img/logos/docker.svg",
        "name": "docker",
        "oss": [
          "Linux",
          "MacOS"
        ],
        "strategies": [
          {
            "name": "daemon",
            "source": "docker"
          },
          {
            "name": "sidecar",
            "source": "file"
          },
          {
            "name": "service",
            "source": "qovery"
          }
        ],
        "title": "Docker"
      }
    }
  },
  "latest_post": {
    "author_github": "https://github.com/lukesteensen",
    "date": "2020-04-09",
    "description": "When we set out to build Qovery, we knew that reliability and performance were\ntwo of our top priorities. We also knew that even the best of intentions would\nnot be enough to make certain those qualities were realized and reflected in our\nusers' production deployments. Since then, we've been continuously evolving and\nexpanding our approach to achieving that level of quality.",
    "id": "how-we-test-qovery",
    "path": "website/blog/2020-04-09-how-we-test-qovery.md",
    "permalink": "https://docs.qovery.com/blog/how-we-test-qovery",
    "tags": [
      "type: post",
      "domain: testing"
    ],
    "title": "How We Test Qovery"
  },
  "latest_release": {
  },
  "post_tags": [
    "type: announcement",
    "domain: platforms",
    "platform: arm",
    "platform: windows",
    "domain: config",
    "domain: networking",
    "platform: kubernetes",
    "domain: transforms",
    "transform: ec2_metadata",
    "domain: sources",
    "source: prometheus",
    "type: post",
    "domain: testing"
  ],
  "posts": [
    {
      "author_github": "https://github.com/lukesteensen",
      "date": "2019-06-28",
      "description": "Today we're very excited to open source the Qovery project! Qovery is a tool for building flexible and robust pipelines for your logs and metrics data. We're still in the early stages, but our goal with Qovery is to dramatically simplify your observability infrastructure while making it easy to get more value from your data.",
      "id": "introducing-qovery",
      "path": "website/blog/2019-06-28-introducing-qovery.md",
      "permalink": "https://docs.qovery.com/blog/introducing-qovery",
      "tags": [
        "type: announcement"
      ],
      "title": "Introducing Qovery"
    },
    {
      "author_github": "https://github.com/binarylogic",
      "date": "2019-11-19",
      "description": "Qovery now supports ARM architectures on the Linux platform! These\narchitectures are widely used in embeded devices and recently started to get\ntraction on servers. To get started, you can follow the installation\ninstructions for your preferred method:",
      "id": "arm-support-on-linux",
      "path": "website/blog/2019-11-19-arm-support-on-linux.md",
      "permalink": "https://docs.qovery.com/blog/arm-support-on-linux",
      "tags": [
        "type: announcement",
        "domain: platforms",
        "platform: arm"
      ],
      "title": "ARMv7 & ARM64 Support on Linux"
    },
    {
      "author_github": "https://github.com/binarylogic",
      "date": "2019-11-21",
      "description": "We're excited to announce that Qovery can now be installed on Windows!\nTo get started, check out the Windows installation instructions\nor head over to the releases section and download the\nappropriate Windows archive. Just like on Linux, installation on Windows is\nquick and easy. Let us know what you think!.",
      "id": "windows-support",
      "path": "website/blog/2019-11-21-windows-support.md",
      "permalink": "https://docs.qovery.com/blog/windows-support",
      "tags": [
        "type: announcement",
        "domain: platforms",
        "platform: windows"
      ],
      "title": "Windows Support Is Here!"
    },
    {
      "author_github": "https://github.com/binarylogic",
      "date": "2019-11-25",
      "description": "Today we're excited to announce beta support for unit testing Qovery\nconfigurations, allowing you to define tests directly within your Qovery\nconfiguration file. These tests are used to assert the output from topologies of\ntransform components given certain input events, ensuring\nthat your configuration behavior does not regress; a very powerful feature for\nmission-critical production pipelines that are collaborated on.",
      "id": "unit-testing-qovery-config-files",
      "path": "website/blog/2019-11-25-unit-testing-qovery-config-files.md",
      "permalink": "https://docs.qovery.com/blog/unit-testing-qovery-config-files",
      "tags": [
        "type: announcement",
        "domain: config"
      ],
      "title": "Unit Testing Your Qovery Config Files"
    },
    {
      "author_github": "https://github.com/Jeffail",
      "date": "2019-12-13",
      "description": "We're modern progressive parents and aren't about to tell Qovery who it can and\ncan't hang out with. As such, we're now allowing you to specify custom DNS\nservers in your configs.",
      "id": "custom-dns",
      "path": "website/blog/2019-12-13-custom-dns.md",
      "permalink": "https://docs.qovery.com/blog/custom-dns",
      "tags": [
        "type: announcement",
        "domain: networking"
      ],
      "title": "Use Custom DNS Servers"
    },
    {
      "author_github": "https://github.com/Jeffail",
      "date": "2019-12-14",
      "description": "We're currently experimenting with Kubernetes integration\nThis functionality is undocumented and not yet ready for general use. However,\nwe consider it to be at Alpha stage and suitable for adventurous early adopters\nto try out.",
      "id": "kubernetes-source-alpha",
      "path": "website/blog/2019-12-14-kubernetes-source-alpha.md",
      "permalink": "https://docs.qovery.com/blog/kubernetes-source-alpha",
      "tags": [
        "type: announcement",
        "domain: platforms",
        "platform: kubernetes"
      ],
      "title": "Alpha Kubernetes Source"
    },
    {
      "author_github": "https://github.com/Jeffail",
      "date": "2019-12-16",
      "description": "Are your events the laughing-stock of the data warehouse? Then enrich them with\nour brand spanking new `aws_ec2_metadata` transform.",
      "id": "ec2-metadata",
      "path": "website/blog/2019-12-16-ec2-metadata.md",
      "permalink": "https://docs.qovery.com/blog/ec2-metadata",
      "tags": [
        "type: announcement",
        "domain: transforms",
        "transform: ec2_metadata"
      ],
      "title": "EC2 Metadata Enrichments"
    },
    {
      "author_github": "https://github.com/Jeffail",
      "date": "2020-01-07",
      "description": "We love Prometheus, but we also love options\nand so we've added a `prometheus` source to let you\nsend Prometheus format metrics anywhere you like.",
      "id": "prometheus-source",
      "path": "website/blog/2020-01-07-prometheus-source.md",
      "permalink": "https://docs.qovery.com/blog/prometheus-source",
      "tags": [
        "type: announcement",
        "domain: sources",
        "source: prometheus"
      ],
      "title": "Prometheus Source"
    },
    {
      "author_github": "https://github.com/lukesteensen",
      "date": "2020-04-09",
      "description": "When we set out to build Qovery, we knew that reliability and performance were\ntwo of our top priorities. We also knew that even the best of intentions would\nnot be enough to make certain those qualities were realized and reflected in our\nusers' production deployments. Since then, we've been continuously evolving and\nexpanding our approach to achieving that level of quality.",
      "id": "how-we-test-qovery",
      "path": "website/blog/2020-04-09-how-we-test-qovery.md",
      "permalink": "https://docs.qovery.com/blog/how-we-test-qovery",
      "tags": [
        "type: post",
        "domain: testing"
      ],
      "title": "How We Test Qovery"
    }
  ],
  "releases": {
  },
  "sinks": {
    "aws_cloudwatch_logs": {
      "beta": false,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"aws_cloudwatch_logs\" # required\n  inputs = [\"in\"] # required\n  group_name = \"group-name\" # required\n  region = \"us-east-1\" # required, required when endpoint = \"\"\n  stream_name = \"{{ host }}\" # required\n\n  # Encoding\n  encoding.codec = \"json\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. With CloudWatch, you can collect and access all your performance and operational data in form of logs and metrics from a single platform.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to AWS Cloudwatch.",
        "Dynamically partition logs across CloudWatch groups and streams.",
        "Batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "aws_cloudwatch_logs_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/aws_cloudwatch.svg",
      "name": "aws_cloudwatch_logs",
      "noun": "AWS Cloudwatch Logs",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "AWS"
      ],
      "short_description": "Batches log events to Amazon Web Service's CloudWatch Logs service via the `PutLogEvents` API endpoint.",
      "status": "prod-ready",
      "title": "AWS Cloudwatch Logs",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Amazon Web Service's CloudWatch Logs service via the `PutLogEvents` API endpoint"
    },
    "aws_cloudwatch_metrics": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  type = \"aws_cloudwatch_metrics\" # required\n  inputs = [\"in\"] # required\n  namespace = \"service\" # required\n  region = \"us-east-1\" # required, required when endpoint = \"\""
      },
      "delivery_guarantee": "at_least_once",
      "description": "Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. With CloudWatch, you can collect and access all your performance and operational data in form of logs and metrics from a single platform.",
      "event_types": [
        "metric"
      ],
      "features": [
        "Send metrics to AWS Cloudwatch.",
        "Batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Automatically aggregate metrics at the edge for improved performance."
      ],
      "function_category": "transmit",
      "id": "aws_cloudwatch_metrics_sink",
      "input_types": [
        "metric"
      ],
      "logo_path": "/img/logos/aws_cloudwatch.svg",
      "name": "aws_cloudwatch_metrics",
      "noun": "AWS Cloudwatch Metrics",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "AWS"
      ],
      "short_description": "Streams metric events to Amazon Web Service's CloudWatch Metrics service via the `PutMetricData` API endpoint.",
      "status": "beta",
      "title": "AWS Cloudwatch Metrics",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Amazon Web Service's CloudWatch Metrics service via the `PutMetricData` API endpoint"
    },
    "aws_kinesis_firehose": {
      "beta": false,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"aws_kinesis_firehose\" # required\n  inputs = [\"in\"] # required\n  region = \"us-east-1\" # required, required when endpoint = \"\"\n  stream_name = \"my-stream\" # required\n\n  # Encoding\n  encoding.codec = \"json\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon Elasticsearch Service (Amazon ES), and Splunk.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to AWS Kinesis Firehose.",
        "Batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "aws_kinesis_firehose_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/aws_kinesis_firehose.svg",
      "name": "aws_kinesis_firehose",
      "noun": "AWS Kinesis Firehose",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "AWS"
      ],
      "short_description": "Batches log events to Amazon Web Service's Kinesis Data Firehose via the `PutRecordBatch` API endpoint.",
      "status": "prod-ready",
      "title": "AWS Kinesis Firehose",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Amazon Web Service's Kinesis Data Firehose via the `PutRecordBatch` API endpoint"
    },
    "aws_kinesis_streams": {
      "beta": false,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"aws_kinesis_streams\" # required\n  inputs = [\"in\"] # required\n  region = \"us-east-1\" # required, required when endpoint = \"\"\n  stream_name = \"my-stream\" # required\n\n  # Encoding\n  encoding.codec = \"json\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "Amazon Kinesis Data Streams is a scalable and durable real-time data streaming service that can continuously capture gigabytes of data per second from hundreds of thousands of sources. Making it an excellent candidate for streaming logs and metrics data.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to AWS Kinesis Data Streams.",
        "Batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "aws_kinesis_streams_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/aws_kinesis_streams.svg",
      "name": "aws_kinesis_streams",
      "noun": "AWS Kinesis Data Streams",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "AWS"
      ],
      "short_description": "Batches log events to Amazon Web Service's Kinesis Data Stream service via the `PutRecords` API endpoint.",
      "status": "prod-ready",
      "title": "AWS Kinesis Data Streams",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Amazon Web Service's Kinesis Data Stream service via the `PutRecords` API endpoint"
    },
    "aws_s3": {
      "beta": false,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"aws_s3\" # required\n  inputs = [\"in\"] # required\n  bucket = \"my-bucket\" # required\n  compression = \"gzip\" # required\n  region = \"us-east-1\" # required, required when endpoint = \"\"\n\n  # Encoding\n  encoding.codec = \"ndjson\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "Amazon Simple Storage Service (Amazon S3) is a scalable, high-speed, web-based cloud storage service designed for online backup and archiving of data and applications on Amazon Web Services. It is very commonly used to store log data.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to AWS S3.",
        "Dynamically partition logs across different key prefixes.",
        "Compress and batch data to reduce storage cost and imrpove throughput.",
        "Optionally adjust ACL and encryption settings.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "aws_s3_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/aws_s3.svg",
      "name": "aws_s3",
      "noun": "AWS S3",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "AWS"
      ],
      "short_description": "Batches log events to Amazon Web Service's S3 service via the `PutObject` API endpoint.",
      "status": "prod-ready",
      "title": "AWS S3",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Amazon Web Service's S3 service via the `PutObject` API endpoint"
    },
    "blackhole": {
      "beta": false,
      "config_examples": {
        "toml": "[sinks.out]\n  type = \"blackhole\" # required\n  inputs = [\"in\"] # required\n  print_amount = 1000 # required"
      },
      "delivery_guarantee": "best_effort",
      "description": null,
      "event_types": [
        "log",
        "metric"
      ],
      "features": [
        "Test log throughput.",
        "Print log totals on an interval."
      ],
      "function_category": "test",
      "id": "blackhole_sink",
      "input_types": [
        "log",
        "metric"
      ],
      "logo_path": null,
      "name": "blackhole",
      "noun": "blackhole",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [

      ],
      "short_description": "Streams log and metric events to a blackhole that simply discards data, designed for testing and benchmarking purposes.",
      "status": "prod-ready",
      "title": "Blackhole",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "a blackhole that simply discards data, designed for testing and benchmarking purposes"
    },
    "clickhouse": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  type = \"clickhouse\" # required\n  inputs = [\"in\"] # required\n  host = \"http://localhost:8123\" # required\n  table = \"mytable\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to ClickHouse.",
        "Compress and batch data to maximize throughput.",
        "Encode timestamps to ClickHouse supported formats.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "clickhouse_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/clickhouse.svg",
      "name": "clickhouse",
      "noun": "Clickhouse",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "Yandex"
      ],
      "short_description": "Batches log events to Clickhouse via the `HTTP` Interface.",
      "status": "beta",
      "title": "Clickhouse",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Clickhouse via the `HTTP` Interface"
    },
    "console": {
      "beta": false,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"console\" # required\n  inputs = [\"in\"] # required\n\n  # Encoding\n  encoding.codec = \"json\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": null,
      "event_types": [
        "log",
        "metric"
      ],
      "features": [
        "Print logs to STDOUT or STDERR.",
        "Encode logs to JSON or text."
      ],
      "function_category": "test",
      "id": "console_sink",
      "input_types": [
        "log",
        "metric"
      ],
      "logo_path": null,
      "name": "console",
      "noun": "the console",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [

      ],
      "short_description": "Streams log and metric events to standard output streams, such as STDOUT and STDERR.",
      "status": "prod-ready",
      "title": "Console",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "standard output streams, such as STDOUT and STDERR"
    },
    "datadog_logs": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"datadog_logs\" # required\n  inputs = [\"in\"] # required\n  api_key = \"${DATADOG_API_KEY_ENV_VAR}\" # required\n\n  # Encoding\n  encoding.codec = \"json\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": "Datadog is a monitoring service for cloud-scale applications, providing monitoring of servers, databases, tools, and services, through a SaaS-based data analytics platform.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to DataDog.",
        "Automatically map common fields to Datadog's reserved fields.",
        "Compress and batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "datadog_logs_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/datadog.svg",
      "name": "datadog_logs",
      "noun": "Datadog Logs",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "Datadog"
      ],
      "short_description": "Streams log events to Datadog's logs via the TCP endpoint.",
      "status": "beta",
      "title": "Datadog Logs",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Datadog's logs via the TCP endpoint"
    },
    "datadog_metrics": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  type = \"datadog_metrics\" # required\n  inputs = [\"in\"] # required\n  api_key = \"${DATADOG_API_KEY}\" # required\n  namespace = \"service\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": "Datadog is a monitoring service for cloud-scale applications, providing monitoring of servers, databases, tools, and services, through a SaaS-based data analytics platform.",
      "event_types": [
        "metric"
      ],
      "features": [
        "Send metrics to Datadog.",
        "Batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Automatically aggregate metrics at the edge for improved performance."
      ],
      "function_category": "transmit",
      "id": "datadog_metrics_sink",
      "input_types": [
        "metric"
      ],
      "logo_path": "/img/logos/datadog.svg",
      "name": "datadog_metrics",
      "noun": "Datadog Metrics",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "Datadog"
      ],
      "short_description": "Batches metric events to Datadog's metrics service using HTTP API.",
      "status": "beta",
      "title": "Datadog Metrics",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Datadog's metrics service using HTTP API"
    },
    "elasticsearch": {
      "beta": false,
      "config_examples": {
        "toml": "[sinks.out]\n  type = \"elasticsearch\" # required\n  inputs = [\"in\"] # required"
      },
      "delivery_guarantee": "best_effort",
      "description": "Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. As a result, it is very commonly used to store and analyze log data. It ships with Kibana which is a simple interface for visualizing and exploring data in Elasticsearch.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to Elasticsearch (AWS, Elastic Cloud, self-hosted, etc).",
        "Batch data to maximize throughput.",
        "Dynamically partition logs across indexes.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "elasticsearch_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/elasticsearch.svg",
      "name": "elasticsearch",
      "noun": "Elasticsearch",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "AWS",
        "Elastic"
      ],
      "short_description": "Batches log events to Elasticsearch via the `_bulk` API endpoint.",
      "status": "prod-ready",
      "title": "Elasticsearch",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Elasticsearch via the `_bulk` API endpoint"
    },
    "file": {
      "beta": false,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"file\" # required\n  inputs = [\"in\"] # required\n  path = \"qovery-%Y-%m-%d.log\" # required\n\n  # Encoding\n  encoding.codec = \"ndjson\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [
        "Write logs to files.",
        "Dynamically partition logs across multiple files."
      ],
      "function_category": "transmit",
      "id": "file_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/file.svg",
      "name": "file",
      "noun": "a file",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [

      ],
      "short_description": "Streams log events to a file.",
      "status": "prod-ready",
      "title": "File",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "a file"
    },
    "gcp_cloud_storage": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"gcp_cloud_storage\" # required\n  inputs = [\"in\"] # required\n  bucket = \"my-bucket\" # required\n  compression = \"gzip\" # required\n  credentials_path = \"/path/to/credentials.json\" # required\n\n  # Encoding\n  encoding.codec = \"ndjson\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "Google Cloud Storage is a RESTful online file storage web service for storing and accessing data on Google Cloud Platform infrastructure. The service combines the performance and scalability of Google's cloud with advanced security and sharing capabilities. This makes it a prime candidate for log data.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to GCP Cloud Storage.",
        "Leverage any of GCP's IAM strategies.",
        "Confifgure object sizes to reduce request cost.",
        "Dynamically partition logs across different key prefixes.",
        "Optionally compress data to reduce storage cost.",
        "Control object-level ACL.",
        "Choose different storage classes for cost control.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "gcp_cloud_storage_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/gcp_cloud_storage.svg",
      "name": "gcp_cloud_storage",
      "noun": "GCP Cloud Storage (GCS)",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "GCP"
      ],
      "short_description": "Batches log events to Google Cloud Platform's Cloud Storage service via the XML Interface.",
      "status": "beta",
      "title": "GCP Cloud Storage (GCS)",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Google Cloud Platform's Cloud Storage service via the XML Interface"
    },
    "gcp_pubsub": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  type = \"gcp_pubsub\" # required\n  inputs = [\"in\"] # required\n  project = \"qovery-123456\" # required\n  topic = \"this-is-a-topic\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": "GCP Pub/Sub is a fully-managed real-time messaging service that allows you to send and receive messages between independent applications on the Google Cloud Platform.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to GCP PubSub.",
        "Leverage any of GCP's IAM strategies.",
        "Batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "gcp_pubsub_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/gcp_pubsub.svg",
      "name": "gcp_pubsub",
      "noun": "GCP PubSub",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "GCP"
      ],
      "short_description": "Batches log events to Google Cloud Platform's Pubsub service via the REST Interface.",
      "status": "beta",
      "title": "GCP PubSub",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Google Cloud Platform's Pubsub service via the REST Interface"
    },
    "gcp_stackdriver_logs": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  type = \"gcp_stackdriver_logs\" # required\n  inputs = [\"in\"] # required\n  credentials_path = \"/path/to/credentials.json\" # required\n  log_id = \"qovery-logs\" # required\n  project_id = \"qovery-123456\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": "Stackdriver is Google Cloud's embedded observability suite designed to monitor, troubleshoot, and improve cloud infrastructure, software, and application performance. Stackdriver enables you to efficiently build and run workloads, keeping applications performant and available.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to GCP Stackdriver.",
        "Leverage any of GCP's IAM strategies.",
        "Batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "gcp_stackdriver_logs_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/gcp_stackdriver.svg",
      "name": "gcp_stackdriver_logs",
      "noun": "GCP Stackdriver Logs",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "GCP"
      ],
      "short_description": "Batches log events to Google Cloud Platform's Stackdriver Logging service via the REST Interface.",
      "status": "beta",
      "title": "GCP Stackdriver Logs",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Google Cloud Platform's Stackdriver Logging service via the REST Interface"
    },
    "honeycomb": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  type = \"honeycomb\" # required\n  inputs = [\"in\"] # required\n  api_key = \"${HONEYCOMB_API_KEY}\" # required\n  dataset = \"my-honeycomb-dataset\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "Honeycomb provides full stack observability—designed for high cardinality data and collaborative problem solving, enabling engineers to deeply understand and debug production software together.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send structured logs to the Honeycomb observability service.",
        "Batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "honeycomb_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/honeycomb.svg",
      "name": "honeycomb",
      "noun": "Honeycomb",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "Honeycomb"
      ],
      "short_description": "Batches log events to Honeycomb via the batch events API.",
      "status": "beta",
      "title": "Honeycomb",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Honeycomb via the batch events API"
    },
    "http": {
      "beta": false,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"http\" # required\n  inputs = [\"in\"] # required\n  uri = \"https://10.22.212.22:9000/endpoint\" # required\n\n  # Encoding\n  encoding.codec = \"json\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs over the HTTP protocol.",
        "Batch and compress data to maximize throughput.",
        "Optionally set custom headers.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "http_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/http.svg",
      "name": "http",
      "noun": "an HTTP endpoint",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [

      ],
      "short_description": "Batches log events to a generic HTTP endpoint.",
      "status": "prod-ready",
      "title": "HTTP",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "a generic HTTP endpoint"
    },
    "humio_logs": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  type = \"humio_logs\" # required\n  inputs = [\"in\"] # required\n  token = \"${HUMIO_TOKEN}\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "Humio is a time-series logging and aggregation platform for unrestricted, comprehensive event analysis, On-Premises or in the Cloud. With 1TB/day of raw log ingest/node, in-memory stream processing, and live, shareable dashboards and alerts, you can instantly and in real-time explore, monitor, and visualize any system’s data.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to the Humio logging service.",
        "Batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "humio_logs_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/humio.svg",
      "name": "humio_logs",
      "noun": "Humio",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "Humio"
      ],
      "short_description": "Batches log events to Humio via the HEC API.",
      "status": "beta",
      "title": "Humio Logs",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Humio via the HEC API"
    },
    "influxdb_metrics": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"influxdb_metrics\" # required\n  inputs = [\"in\"] # required\n  endpoint = \"http://localhost:8086/\" # required\n  namespace = \"service\" # required\n  bucket = \"qovery-bucket\" # required\n  database = \"qovery-database\" # required\n\n  # Auth\n  org = \"my-org\" # required\n  token = \"${INFLUXDB_TOKEN}\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "InfluxDB is an open-source time series database developed by InfluxData. It is written in Go and optimized for fast, high-availability storage and retrieval of time series data in fields such as operations monitoring, application metrics, Internet of Things sensor data, and real-time analytics.",
      "event_types": [
        "metric"
      ],
      "features": [
        "Send metrics to InfluxDB v1 or v2.",
        "Batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Automatically aggregate metrics at the edge for improved performance."
      ],
      "function_category": "transmit",
      "id": "influxdb_metrics_sink",
      "input_types": [
        "metric"
      ],
      "logo_path": "/img/logos/influxdb.svg",
      "name": "influxdb_metrics",
      "noun": "InfluxDB",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "InfluxData"
      ],
      "short_description": "Batches metric events to InfluxDB using v1 or v2 HTTP API.",
      "status": "beta",
      "title": "InfluxDB Metrics",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "InfluxDB using v1 or v2 HTTP API"
    },
    "kafka": {
      "beta": false,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"kafka\" # required\n  inputs = [\"in\"] # required\n  bootstrap_servers = \"10.14.22.123:9092,10.14.23.332:9092\" # required\n  key_field = \"user_id\" # required\n  topic = \"topic-1234\" # required\n\n  # Encoding\n  encoding.codec = \"json\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "Apache Kafka is an open source project for a distributed publish-subscribe messaging system rethought as a distributed commit log. Kafka stores messages in topics that are partitioned and replicated across multiple brokers in a cluster. Producers send messages to topics from which consumers read. This makes it an excellent candidate for durably storing logs and metrics data.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to Kafka.",
        "Leverage any of AWS' IAM strategies.",
        "Optionally compress data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "kafka_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/kafka.svg",
      "name": "kafka",
      "noun": "Kafka",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "AWS",
        "Confluent"
      ],
      "short_description": "Streams log events to Apache Kafka via the Kafka protocol.",
      "status": "prod-ready",
      "title": "Kafka",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Apache Kafka via the Kafka protocol"
    },
    "logdna": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  type = \"logdna\" # required\n  inputs = [\"in\"] # required\n  api_key = \"${LOGDNA_API_KEY}\" # required\n  hostname = \"${HOSTNAME}\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": "LogDNA is a log management system that allows engineering and DevOps to aggregate all system, server, and application logs into one platform. Collect, monitor, store, tail, and search application logs in with one command-line or web interface.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to the LogDNA logging service.",
        "Batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "logdna_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/logdna.svg",
      "name": "logdna",
      "noun": "LogDNA",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "LogDNA"
      ],
      "short_description": "Batches log events to LogDna's HTTP Ingestion API.",
      "status": "beta",
      "title": "LogDNA",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "LogDna's HTTP Ingestion API"
    },
    "loki": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"loki\" # required\n  inputs = [\"in\"] # required\n  endpoint = \"http://localhost:3100\" # required\n\n  # Labels\n  labels.key = \"value\" # example\n  labels.key = \"{{ event_field }}\" # example"
      },
      "delivery_guarantee": "best_effort",
      "description": "Loki is a horizontally-scalable, highly-available, multi-tenant log aggregation system inspired by Prometheus. It is designed to be very cost effective and easy to operate. It does not index the contents of the logs, but rather a set of labels for each log stream.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send structured logs to the Loki logging service.",
        "Batch data to maximize throughput.",
        "Set custom labels to be added to all log data.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "loki_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/loki.svg",
      "name": "loki",
      "noun": "Loki",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "Grafana"
      ],
      "short_description": "Batches log events to Loki.",
      "status": "beta",
      "title": "Loki",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Loki"
    },
    "new_relic_logs": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  type = \"new_relic_logs\" # required\n  inputs = [\"in\"] # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "New Relic is a San Francisco, California-based technology company which develops cloud-based software to help website and application owners track the performances of their services.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to the New Relic logging service.",
        "Batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "new_relic_logs_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/new_relic.svg",
      "name": "new_relic_logs",
      "noun": "New Relic",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "New Relic"
      ],
      "short_description": "Batches log events to New Relic's log service via their log API.",
      "status": "beta",
      "title": "New Relic Logs",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "New Relic's log service via their log API"
    },
    "papertrail": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"papertrail\" # required\n  inputs = [\"in\"] # required\n  endpoint = \"logs.papertrailapp.com:12345\" # required\n\n  # Encoding\n  encoding.codec = \"json\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": "Papertrail is a web-based log aggregation application used by developers and IT team to search and view logs in real time.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to the Papertrail logging service.",
        "Batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "papertrail_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/papertrail.svg",
      "name": "papertrail",
      "noun": "Papertrail",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "Papertrail"
      ],
      "short_description": "Streams log events to Papertrail via Syslog.",
      "status": "beta",
      "title": "Papertrail",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Papertrail via Syslog"
    },
    "prometheus": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  type = \"prometheus\" # required\n  inputs = [\"in\"] # required\n  address = \"0.0.0.0:9598\" # required\n  namespace = \"service\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": "Prometheus is a pull-based monitoring system that scrapes metrics from configured endpoints, stores them efficiently, and supports a powerful query language to compose dynamic information from a variety of otherwise unrelated data points.",
      "event_types": [
        "metric"
      ],
      "features": [
        "Expose an endpoint that Prometheus can scrape for metrics data.",
        "Automatically aggregate metrics at the edge for improved performance."
      ],
      "function_category": "transmit",
      "id": "prometheus_sink",
      "input_types": [
        "metric"
      ],
      "logo_path": "/img/logos/prometheus.svg",
      "name": "prometheus",
      "noun": "Prometheus",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [

      ],
      "short_description": "Exposes metric events to Prometheus metrics service.",
      "status": "beta",
      "title": "Prometheus",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Prometheus metrics service"
    },
    "pulsar": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"pulsar\" # required\n  inputs = [\"in\"] # required\n  address = \"127.0.0.1:6650\" # required\n  topic = \"topic-1234\" # required\n\n  # Encoding\n  encoding.codec = \"json\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "Pulsar is a multi-tenant, high-performance solution for server-to-server messaging. Pulsar was originally developed by Yahoo, it is under the stewardship of the Apache Software Foundation. It is an excellent tool for streaming logs and metrics data.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to Apache Pulsar.",
        "Stream data in a real-time fashion.",
        "Automatically retry failed requests, with backoff."
      ],
      "function_category": "transmit",
      "id": "pulsar_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/pulsar.svg",
      "name": "pulsar",
      "noun": "Apache Pulsar",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [

      ],
      "short_description": "Streams log events to Apache Pulsar via the Pulsar protocol.",
      "status": "beta",
      "title": "Apache Pulsar",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Apache Pulsar via the Pulsar protocol"
    },
    "qovery": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  type = \"qovery\" # required\n  inputs = [\"in\"] # required\n  address = \"92.12.333.224:5000\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": null,
      "event_types": [
        "log",
        "metric"
      ],
      "features": [
        "Send data to another downstream Qovery instance."
      ],
      "function_category": "transmit",
      "id": "qovery_sink",
      "input_types": [
        "log",
        "metric"
      ],
      "logo_path": "/img/logos/qovery.svg",
      "name": "qovery",
      "noun": "another Qovery instance",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [

      ],
      "short_description": "Streams log and metric events to another downstream `qovery` source.",
      "status": "beta",
      "title": "Qovery",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "another downstream `qovery` source"
    },
    "sematext_logs": {
      "beta": true,
      "config_examples": {
        "toml": "[sinks.out]\n  type = \"sematext_logs\" # required\n  inputs = [\"in\"] # required\n  token = \"${SEMATEXT_TOKEN}\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": "Sematext is a hosted monitoring platform based on Elasticsearch. Providing powerful monitoring and management solutions to monitor and observe your apps in real-time.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to the Sematext monitoring service.",
        "Batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "sematext_logs_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/sematext.svg",
      "name": "sematext_logs",
      "noun": "Sematext",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "Sematext"
      ],
      "short_description": "Batches log events to Sematext via the Elasticsearch API.",
      "status": "beta",
      "title": "Sematext Logs",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "Sematext via the Elasticsearch API"
    },
    "socket": {
      "beta": false,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"socket\" # required\n  inputs = [\"in\"] # required\n  address = \"92.12.333.224:5000\" # required, required when mode = \"tcp\" or mode = \"udp\"\n  mode = \"tcp\" # required\n  path = \"/path/to/socket\" # required, required when mode = \"unix\"\n\n  # Encoding\n  encoding.codec = \"json\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [
        "Stream logs over a TCP, UDP, or Unix socket.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "socket_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/socket.svg",
      "name": "socket",
      "noun": "a TCP, UDP, or UDS socket",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [

      ],
      "short_description": "Streams log events to a socket, such as a TCP, UDP, or UDS socket.",
      "status": "prod-ready",
      "title": "Socket",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "a socket, such as a TCP, UDP, or UDS socket"
    },
    "splunk_hec": {
      "beta": false,
      "config_examples": {
        "toml": "[sinks.out]\n  # General\n  type = \"splunk_hec\" # required\n  inputs = [\"in\"] # required\n  host = \"http://my-splunk-host.com\" # required\n  token = \"${SPLUNK_HEC_TOKEN}\" # required\n\n  # Encoding\n  encoding.codec = \"json\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "The Splunk HTTP Event Collector (HEC) is a fast and efficient way to send data to Splunk Enterprise and Splunk Cloud. Notably, HEC enables you to send data over HTTP (or HTTPS) directly to Splunk Enterprise or Splunk Cloud from your application.",
      "event_types": [
        "log"
      ],
      "features": [
        "Send logs to a Splunk HTTP event collector.",
        "Customize which fields should be added to the Splunk index.",
        "Batch data to maximize throughput.",
        "Automatically retry failed requests, with backoff.",
        "Buffer your data in-memory or on-disk for performance and durability."
      ],
      "function_category": "transmit",
      "id": "splunk_hec_sink",
      "input_types": [
        "log"
      ],
      "logo_path": "/img/logos/splunk_hec.svg",
      "name": "splunk_hec",
      "noun": "a Splunk HEC",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [
        "Splunk"
      ],
      "short_description": "Batches log events to a Splunk's HTTP Event Collector.",
      "status": "prod-ready",
      "title": "Splunk HEC",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "a Splunk's HTTP Event Collector"
    },
    "statsd": {
      "beta": false,
      "config_examples": {
        "toml": "[sinks.out]\n  type = \"statsd\" # required\n  inputs = [\"in\"] # required\n  namespace = \"service\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": "StatsD is a standard and, by extension, a set of tools that can be used to send, collect, and aggregate custom metrics from any application. Originally, StatsD referred to a daemon written by Etsy in Node.",
      "event_types": [
        "metric"
      ],
      "features": [
        "Stream metrics over a StatsD UDP protocol.",
        "Automatically aggregate metrics at the edge for improved performance."
      ],
      "function_category": "transmit",
      "id": "statsd_sink",
      "input_types": [
        "metric"
      ],
      "logo_path": "/img/logos/statsd.svg",
      "name": "statsd",
      "noun": "Statsd",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "service_providers": [

      ],
      "short_description": "Streams metric events to StatsD metrics service.",
      "status": "prod-ready",
      "title": "Statsd",
      "type": "sink",
      "unsupported_operating_systems": [

      ],
      "write_to_description": "StatsD metrics service"
    }
  },
  "sources": {
    "docker": {
      "beta": true,
      "config_examples": {
        "toml": "[sources.in]\n  type = \"docker\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": "Docker is an open platform for developing, shipping, and running\napplications and services. Docker enables you to separate your services from your infrastructure so you can ship quickly. With Docker, you can manage your infrastructure in the same ways you manage your services. By taking advantage of Docker’s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.",
      "event_types": [
        "log"
      ],
      "features": [
        "Collect Docker container logs.",
        "Filter which containers you collect them from.",
        "Automatically merge logs that Docker splits.",
        "Enrich your logs with useful Docker context."
      ],
      "function_category": "collect",
      "id": "docker_source",
      "logo_path": "/img/logos/docker.svg",
      "name": "docker",
      "noun": "Docker",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Ingests data through the Docker engine daemon and outputs log events.",
      "status": "beta",
      "through_description": "the Docker engine daemon",
      "title": "Docker",
      "type": "source",
      "unsupported_operating_systems": [

      ]
    },
    "file": {
      "beta": false,
      "config_examples": {
        "toml": "[sources.in]\n  type = \"file\" # required\n  include = [\"/var/log/nginx/*.log\"] # required"
      },
      "delivery_guarantee": "best_effort",
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [
        "Tail one or more files.",
        "Automatically discover new files with glob patterns.",
        "Merge multi-line logs into one event.",
        "Checkpoint your position to ensure data is not lost between restarts.",
        "Enrich your logs with useful file and host-level context."
      ],
      "function_category": "collect",
      "id": "file_source",
      "logo_path": "/img/logos/file.svg",
      "name": "file",
      "noun": "a file",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Ingests data through one or more local files and outputs log events.",
      "status": "prod-ready",
      "through_description": "one or more local files",
      "title": "File",
      "type": "source",
      "unsupported_operating_systems": [

      ]
    },
    "http": {
      "beta": true,
      "config_examples": {
        "toml": "[sources.in]\n  type = \"http\" # required\n  address = \"0.0.0.0:80\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [
        "Accept log data over HTTP.",
        "Decode JSON, NDJSON, and text.",
        "Enrich your logs with select HTTP headers."
      ],
      "function_category": "receive",
      "id": "http_source",
      "logo_path": "/img/logos/http.svg",
      "name": "http",
      "noun": "HTTP",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Ingests data through the HTTP protocol and outputs log events.",
      "status": "beta",
      "through_description": "the HTTP protocol",
      "title": "HTTP",
      "type": "source",
      "unsupported_operating_systems": [

      ]
    },
    "journald": {
      "beta": true,
      "config_examples": {
        "toml": "[sources.in]\n  type = \"journald\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "Journald is a utility for accessing log data across a variety of system services. It was introduce with Systemd to help system administrator collect, access, and route log data.",
      "event_types": [
        "log"
      ],
      "features": [
        "Collect Journald/Systemd logs.",
        "Filter which Systemd units you collect them from.",
        "Checkpoint your position to ensure data is not lost between restarts.",
        "Enrich your logs with useful Systemd context."
      ],
      "function_category": "collect",
      "id": "journald_source",
      "logo_path": "/img/logos/journald.svg",
      "name": "journald",
      "noun": "Journald",
      "operating_systems": [
        "Linux"
      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Ingests data through Systemd's Journald utility and outputs log events.",
      "status": "beta",
      "through_description": "Systemd's Journald utility",
      "title": "Journald",
      "type": "source",
      "unsupported_operating_systems": [
        "MacOS",
        "Windows"
      ]
    },
    "kafka": {
      "beta": true,
      "config_examples": {
        "toml": "[sources.in]\n  type = \"kafka\" # required\n  bootstrap_servers = \"10.14.22.123:9092,10.14.23.332:9092\" # required\n  group_id = \"consumer-group-name\" # required\n  topics = [\"^(prefix1|prefix2)-.+\", \"topic-1\", \"topic-2\"] # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "Apache Kafka is an open source project for a distributed publish-subscribe messaging system rethought as a distributed commit log. Kafka stores messages in topics that are partitioned and replicated across multiple brokers in a cluster. Producers send messages to topics from which consumers read. This makes it an excellent candidate for durably storing logs and metrics data.",
      "event_types": [
        "log"
      ],
      "features": [
        "Consume one or more Kafka topics.",
        "Checkpoint your position to ensure data is not lost between restarts.",
        "Enrich your logs with useful Kafka context."
      ],
      "function_category": "collect",
      "id": "kafka_source",
      "logo_path": "/img/logos/kafka.svg",
      "name": "kafka",
      "noun": "Kafka",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "output_types": [
        "log"
      ],
      "service_providers": [
        "AWS",
        "Confluent"
      ],
      "short_description": "Ingests data through Kafka and outputs log events.",
      "status": "beta",
      "through_description": "Kafka",
      "title": "Kafka",
      "type": "source",
      "unsupported_operating_systems": [

      ]
    },
    "logplex": {
      "beta": true,
      "config_examples": {
        "toml": "[sources.in]\n  type = \"logplex\" # required\n  address = \"0.0.0.0:80\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "Heroku’s Logplex router is responsible for collating and distributing the log entries generated by Heroku apps and other components of the Heroku platform. It makes these entries available through the Logplex public API and the Heroku command-line tool.",
      "event_types": [
        "log"
      ],
      "features": [
        "Accept Heroku Logplex data over HTTP.",
        "Automatically parse incoming data into structured events."
      ],
      "function_category": "receive",
      "id": "logplex_source",
      "logo_path": "/img/logos/logplex.svg",
      "name": "logplex",
      "noun": "Heroku Logplex",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "output_types": [
        "log"
      ],
      "service_providers": [
        "Heroku"
      ],
      "short_description": "Ingests data through the Heroku Logplex HTTP Drain protocol and outputs log events.",
      "status": "beta",
      "through_description": "the Heroku Logplex HTTP Drain protocol",
      "title": "Heroku Logplex",
      "type": "source",
      "unsupported_operating_systems": [

      ]
    },
    "prometheus": {
      "beta": true,
      "config_examples": {
        "toml": "[sources.in]\n  type = \"prometheus\" # required\n  hosts = [\"http://localhost:9090\"] # required"
      },
      "delivery_guarantee": "best_effort",
      "description": "Prometheus is a pull-based monitoring system that scrapes metrics from configured endpoints, stores them efficiently, and supports a powerful query language to compose dynamic information from a variety of otherwise unrelated data points.",
      "event_types": [
        "metric"
      ],
      "features": [
        "Scrape one or more Prometheus endpoints.",
        "Ingest all Prometheus metric types.",
        "Automatically parse metrics into a lossless interoperable data model."
      ],
      "function_category": "receive",
      "id": "prometheus_source",
      "logo_path": "/img/logos/prometheus.svg",
      "name": "prometheus",
      "noun": "Prometheus",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "output_types": [
        "metric"
      ],
      "service_providers": [

      ],
      "short_description": "Ingests data through the Prometheus text exposition format and outputs metric events.",
      "status": "beta",
      "through_description": "the Prometheus text exposition format",
      "title": "Prometheus",
      "type": "source",
      "unsupported_operating_systems": [

      ]
    },
    "qovery": {
      "beta": true,
      "config_examples": {
        "toml": "[sources.in]\n  type = \"qovery\" # required\n  address = \"0.0.0.0:9000\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": null,
      "event_types": [
        "log",
        "metric"
      ],
      "features": [
        "Accept data from another upstream Qovery instance."
      ],
      "function_category": "receive",
      "id": "qovery_source",
      "logo_path": "/img/logos/qovery.svg",
      "name": "qovery",
      "noun": "Qovery",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "output_types": [
        "log",
        "metric"
      ],
      "service_providers": [

      ],
      "short_description": "Ingests data through another upstream `qovery` sink and outputs log and metric events.",
      "status": "beta",
      "through_description": "another upstream `qovery` sink",
      "title": "Qovery",
      "type": "source",
      "unsupported_operating_systems": [

      ]
    },
    "socket": {
      "beta": false,
      "config_examples": {
        "toml": "[sources.in]\n  type = \"socket\" # required\n  address = \"0.0.0.0:9000\" # required, required when mode = \"tcp\" or mode = \"udp\"\n  mode = \"tcp\" # required\n  path = \"/path/to/socket\" # required, required when mode = \"unix\""
      },
      "delivery_guarantee": "best_effort",
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [
        "Accept log data over a TCP, UDP, or UDS socket.",
        "Automatically enrich logs with host-level context."
      ],
      "function_category": "receive",
      "id": "socket_source",
      "logo_path": "/img/logos/socket.svg",
      "name": "socket",
      "noun": "a TCP, UDP, or UDS socket",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Ingests data through a socket, such as a TCP, UDP, or UDS socket and outputs log events.",
      "status": "prod-ready",
      "through_description": "a socket, such as a TCP, UDP, or UDS socket",
      "title": "Socket",
      "type": "source",
      "unsupported_operating_systems": [

      ]
    },
    "splunk_hec": {
      "beta": true,
      "config_examples": {
        "toml": "[sources.in]\n  type = \"splunk_hec\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": "The Splunk HTTP Event Collector (HEC) is a fast and efficient way to send data to Splunk Enterprise and Splunk Cloud. Notably, HEC enables you to send data over HTTP (or HTTPS) directly to Splunk Enterprise or Splunk Cloud from your application.",
      "event_types": [
        "log"
      ],
      "features": [
        "Accept log data just like the Splunk HTTP event collector.",
        "Automatically parse incoming data into structured events.",
        "Optionally require authentication on all requests."
      ],
      "function_category": "receive",
      "id": "splunk_hec_source",
      "logo_path": "/img/logos/splunk_hec.svg",
      "name": "splunk_hec",
      "noun": "Splunk HEC",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "output_types": [
        "log"
      ],
      "service_providers": [
        "Splunk"
      ],
      "short_description": "Ingests data through the Splunk HTTP Event Collector protocol and outputs log events.",
      "status": "beta",
      "through_description": "the Splunk HTTP Event Collector protocol",
      "title": "Splunk HEC",
      "type": "source",
      "unsupported_operating_systems": [

      ]
    },
    "statsd": {
      "beta": false,
      "config_examples": {
        "toml": "[sources.in]\n  type = \"statsd\" # required\n  address = \"127.0.0.1:8126\" # required"
      },
      "delivery_guarantee": "best_effort",
      "description": "StatsD is a standard and, by extension, a set of tools that can be used to send, collect, and aggregate custom metrics from any application. Originally, StatsD referred to a daemon written by Etsy in Node.",
      "event_types": [
        "metric"
      ],
      "features": [
        "Accept metrics data over the Statsd UDP protocol.",
        "Automatically parse metrics into a lossless interoperable data model."
      ],
      "function_category": "receive",
      "id": "statsd_source",
      "logo_path": "/img/logos/statsd.svg",
      "name": "statsd",
      "noun": "Statsd",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "output_types": [
        "metric"
      ],
      "service_providers": [

      ],
      "short_description": "Ingests data through the StatsD UDP protocol and outputs metric events.",
      "status": "prod-ready",
      "through_description": "the StatsD UDP protocol",
      "title": "Statsd",
      "type": "source",
      "unsupported_operating_systems": [

      ]
    },
    "stdin": {
      "beta": false,
      "config_examples": {
        "toml": "[sources.in]\n  type = \"stdin\" # required"
      },
      "delivery_guarantee": "at_least_once",
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [
        "Accept new line delimited log data through STDIN.",
        "Automatically enrich logs with host-level context."
      ],
      "function_category": "receive",
      "id": "stdin_source",
      "logo_path": "/img/logos/stdin.svg",
      "name": "stdin",
      "noun": "STDIN",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Ingests data through standard input (STDIN) and outputs log events.",
      "status": "prod-ready",
      "through_description": "standard input (STDIN)",
      "title": "STDIN",
      "type": "source",
      "unsupported_operating_systems": [

      ]
    },
    "syslog": {
      "beta": false,
      "config_examples": {
        "toml": "[sources.in]\n  type = \"syslog\" # required\n  address = \"0.0.0.0:514\" # required, required when mode = \"tcp\" or mode = \"udp\"\n  mode = \"tcp\" # required\n  path = \"/path/to/socket\" # required, required when mode = \"unix\""
      },
      "delivery_guarantee": "best_effort",
      "description": "Syslog stands for System Logging Protocol and is a standard protocol used to send system log or event messages to a specific server, called a syslog server. It is primarily used to collect various device logs from several different machines in a central location for monitoring and review.",
      "event_types": [
        "log"
      ],
      "features": [
        "Accept log data over the Syslog protocol via TCP, UDP, or Unix sockets.",
        "Automatically parse Syslog 3164 and 5424 formats."
      ],
      "function_category": "receive",
      "id": "syslog_source",
      "logo_path": "/img/logos/syslog.svg",
      "name": "syslog",
      "noun": "Syslog",
      "operating_systems": [
        "Linux",
        "MacOS",
        "Windows"
      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Ingests data through the Syslog protocol and outputs log events.",
      "status": "prod-ready",
      "through_description": "the Syslog protocol",
      "title": "Syslog",
      "type": "source",
      "unsupported_operating_systems": [

      ]
    }
  },
  "team": [
    {
      "avatar": "https://github.com/a-rodin.png",
      "bio": "Alexander is a Senior Engineer at <a href=\"https://timber.io\">Timber.io</a> and a member of the <a href=\"/community#team\">core Qovery team</a>. He created <a href=\"https://alexanderrodin.com/qstardict\">QStarDict</a>, a dictionary application written in C++.",
      "github": "https://github.com/a-rodin",
      "id": "alex",
      "keybase": "https://keybase.io/arodin",
      "name": "Alexander"
    },
    {
      "avatar": "https://github.com/hoverbear.png",
      "bio": "Ana is a Senior Engineer at <a href=\"https://timber.io\">Timber.io</a> and member of the <a href=\"/community#team\">core Qovery team</a>. She is actively invovled in the Rust community and contributing to project and organizing meetups.",
      "github": "https://github.com/hoverbear",
      "id": "ana",
      "keybase": "https://keybase.io/hoverbear",
      "name": "Ana",
      "twitter": "https://twitter.com/a_hoverbear"
    },
    {
      "avatar": "https://github.com/Jeffail.png",
      "bio": "Ashley is a Principal Engineer at <a href=\"https://timber.io\">Timber.io</a> and member of the <a href=\"/community#team\">core Qovery team</a>. He created <a href=\"https://github.com/Jeffail/benthos\">Benthos stream processor</a>, a popular stream processor written Go.",
      "github": "https://github.com/Jeffail",
      "id": "ashley",
      "keybase": "https://keybase.io/jeffail",
      "name": "Ashley",
      "twitter": "https://twitter.com/jeffail"
    },
    {
      "avatar": "https://github.com/binarylogic.png",
      "bio": "Ben is the CTO/Co-Founder at <a href=\"https://timber.io\">Timber.io</a> and a member of the <a href=\"/community#team\">core Qovery team</a>. He is an open-source veteran, creating <a href=\"https://github.com/binarylogic/authlogic\">Authlogic</a> over 15 years ago before helping to launch Qovery.",
      "github": "https://github.com/binarylogic",
      "id": "ben",
      "keybase": "https://keybase.io/binarylogic",
      "name": "Ben",
      "twitter": "https://twitter.com/binarylogic"
    },
    {
      "avatar": "https://github.com/timber-bradybot.png",
      "github": "https://github.com/timber-bradybot",
      "id": "brady",
      "name": "Brady"
    },
    {
      "avatar": "https://github.com/bruceg.png",
      "github": "https://github.com/bruceg",
      "id": "bruce",
      "name": "Bruce"
    },
    {
      "avatar": "https://github.com/ktff.png",
      "github": "https://github.com/ktff",
      "id": "kruno",
      "name": "Kruno"
    },
    {
      "avatar": "https://github.com/LucioFranco.png",
      "github": "https://github.com/LucioFranco",
      "id": "lucio",
      "keybase": "https://keybase.io/luciofranco",
      "name": "Lucio",
      "twitter": "https://twitter.com/lucio_d_franco"
    },
    {
      "avatar": "https://github.com/lukesteensen.png",
      "bio": "Luke is a Senior Engineer at <a href=\"https://timber.io\">Timber.io</a> and member of the <a href=\"/community#team\">core Qovery team</a>. Before Timber Luke was an engineer at Braintree, working on parts of their observability pipeline.",
      "github": "https://github.com/lukesteensen",
      "id": "luke",
      "keybase": "https://keybase.io/lukesteensen",
      "name": "Luke",
      "twitter": "https://twitter.com/lukesteensen"
    },
    {
      "avatar": "https://github.com/MOZGIII.png",
      "github": "https://github.com/MOZGIII",
      "id": "mike",
      "keybase": "https://keybase.io/MOZGIII",
      "name": "Mike",
      "twitter": "https://twitter.com/MOZGIII"
    },
    {
      "avatar": "https://github.com/qovery-vic.png",
      "github": "https://github.com/qovery-vic",
      "id": "vic",
      "name": "Vic"
    }
  ],
  "transforms": {
    "add_fields": {
      "beta": false,
      "config_examples": {
        "toml": "[transforms.out]\n  # General\n  type = \"add_fields\" # required\n  inputs = [\"in\"] # required\n\n  # Fields\n  fields.string_field = \"string value\" # example\n  fields.env_var_field = \"${ENV_VAR}\" # example\n  fields.templated_field = \"{{ my_other_field }}\" # example\n  fields.int_field = 1 # example\n  fields.float_field = 1.2 # example\n  fields.bool_field = true # example\n  fields.timestamp_field = 1979-05-27T00:32:00Z # example\n  fields.parent.child_field = \"child_value\" # example\n  fields.list_field = [\"first\", \"second\", \"third\"] # example"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "shape",
      "id": "add_fields_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "add_fields",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to add one or more log fields.",
      "status": "prod-ready",
      "title": "Add Fields",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "add_tags": {
      "beta": false,
      "config_examples": {
        "toml": "[transforms.out]\n  # General\n  type = \"add_tags\" # required\n  inputs = [\"in\"] # required\n\n  # Tags\n  tags.static_tag = \"my value\" # example\n  tags.env_tag = \"${ENV_VAR}\" # example"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "metric"
      ],
      "features": [

      ],
      "function_category": "shape",
      "id": "add_tags_transform",
      "inpuut_types": [
        "metric"
      ],
      "logo_path": null,
      "name": "add_tags",
      "operating_systems": [

      ],
      "output_types": [
        "metric"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts metric events and allows you to add one or more metric tags.",
      "status": "prod-ready",
      "title": "Add Tags",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "ansi_stripper": {
      "beta": false,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"ansi_stripper\" # required\n  inputs = [\"in\"] # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "sanitize",
      "id": "ansi_stripper_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "ansi_stripper",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to strips ANSI escape sequences from the specified field.",
      "status": "prod-ready",
      "title": "ANSI Stripper",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "aws_ec2_metadata": {
      "beta": true,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"aws_ec2_metadata\" # required\n  inputs = [\"in\"] # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "enrich",
      "id": "aws_ec2_metadata_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "aws_ec2_metadata",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to enrich logs with AWS EC2 instance metadata.",
      "status": "beta",
      "title": "AWS EC2 Metadata",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "coercer": {
      "beta": false,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"coercer\" # required\n  inputs = [\"in\"] # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "parse",
      "id": "coercer_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "coercer",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to coerce log fields into fixed types.",
      "status": "prod-ready",
      "title": "Coercer",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "concat": {
      "beta": true,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"concat\" # required\n  inputs = [\"in\"] # required\n  items = [\"first[..3]\", \"second[-5..]\", \"third[3..6]\"] # required\n  target = \"root_field_name\" # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "shape",
      "id": "concat_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "concat",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to concat (substrings) of other fields to a new one.",
      "status": "beta",
      "title": "Concat",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "dedupe": {
      "beta": false,
      "config_examples": {
        "toml": "[transforms.out]\n  # General\n  type = \"dedupe\" # required\n  inputs = [\"in\"] # required\n\n  # Fields"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "filter",
      "id": "dedupe_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "dedupe",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to prevent duplicate Events from being outputted by using an LRU cache.",
      "status": "prod-ready",
      "title": "Dedupe events",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "filter": {
      "beta": true,
      "config_examples": {
        "toml": "[transforms.out]\n  # General\n  type = \"filter\" # required\n  inputs = [\"in\"] # required\n\n  # Condition"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log",
        "metric"
      ],
      "features": [

      ],
      "function_category": "filter",
      "id": "filter_transform",
      "inpuut_types": [
        "log",
        "metric"
      ],
      "logo_path": null,
      "name": "filter",
      "operating_systems": [

      ],
      "output_types": [
        "log",
        "metric"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log and metric events and allows you to select events based on a set of logical conditions.",
      "status": "beta",
      "title": "Filter",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "geoip": {
      "beta": true,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"geoip\" # required\n  inputs = [\"in\"] # required\n  database = \"/path/to/GeoLite2-City.mmdb\" # required\n  source = \"ip_address\" # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "enrich",
      "id": "geoip_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "geoip",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to enrich events with geolocation data from the MaxMind GeoIP2 and GeoLite2 city databases.",
      "status": "beta",
      "title": "GeoIP",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "grok_parser": {
      "beta": false,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"grok_parser\" # required\n  inputs = [\"in\"] # required\n  pattern = \"%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:message}\" # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "parse",
      "id": "grok_parser_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "grok_parser",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to parse a log field value with Grok.",
      "status": "prod-ready",
      "title": "Grok Parser",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "json_parser": {
      "beta": false,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"json_parser\" # required\n  inputs = [\"in\"] # required\n  drop_invalid = true # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "parse",
      "id": "json_parser_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "json_parser",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to parse a log field value as JSON.",
      "status": "prod-ready",
      "title": "JSON Parser",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "kubernetes_pod_metadata": {
      "beta": true,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"kubernetes_pod_metadata\" # required\n  inputs = [\"in\"] # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "enrich",
      "id": "kubernetes_pod_metadata_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "kubernetes_pod_metadata",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to enrich Kubernetes logs with Pod metadata.",
      "status": "beta",
      "title": "Kubernetes Pod Metadata",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "log_to_metric": {
      "beta": false,
      "config_examples": {
        "toml": "[transforms.out]\n  # General\n  type = \"log_to_metric\" # required\n  inputs = [\"in\"] # required\n\n  # Metrics\n  metrics.field = \"duration\" # required\n  metrics.name = \"duration_total\" # required\n  metrics.type = \"counter\" # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log",
        "metric"
      ],
      "features": [

      ],
      "function_category": "convert",
      "id": "log_to_metric_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "log_to_metric",
      "operating_systems": [

      ],
      "output_types": [
        "metric"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to convert logs into one or more metrics.",
      "status": "prod-ready",
      "title": "Log to Metric",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "logfmt_parser": {
      "beta": true,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"logfmt_parser\" # required\n  inputs = [\"in\"] # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "parse",
      "id": "logfmt_parser_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "logfmt_parser",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to parse a log field's value in the logfmt format.",
      "status": "beta",
      "title": "Logfmt Parser",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "lua": {
      "beta": true,
      "config_examples": {
        "toml": "[transforms.out]\n  # General\n  type = \"lua\" # required\n  inputs = [\"in\"] # required\n  version = \"2\" # required\n\n  # Hooks\n  hooks.process = \"\"\"\n  function (event, emit)\n    event.log.field = \"value\" -- set value of a field\n    event.log.another_field = nil -- remove field\n    event.log.first, event.log.second = nil, event.log.first -- rename field\n\n    -- Very important! Emit the processed event.\n    emit(event)\n  end\n  \"\"\""
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log",
        "metric"
      ],
      "features": [

      ],
      "function_category": "program",
      "id": "lua_transform",
      "inpuut_types": [
        "log",
        "metric"
      ],
      "logo_path": null,
      "name": "lua",
      "operating_systems": [

      ],
      "output_types": [
        "log",
        "metric"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log and metric events and allows you to transform events with a full embedded Lua engine.",
      "status": "beta",
      "title": "Lua",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "merge": {
      "beta": true,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"merge\" # required\n  inputs = [\"in\"] # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "aggregate",
      "id": "merge_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "merge",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to merge partial log events into a single event.",
      "status": "beta",
      "title": "Merge",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "regex_parser": {
      "beta": false,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"regex_parser\" # required\n  inputs = [\"in\"] # required\n  regex = \"^(?P<timestamp>[\\\\w\\\\-:\\\\+]+) (?P<level>\\\\w+) (?P<message>.*)$\" # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "parse",
      "id": "regex_parser_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "regex_parser",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to parse a log field's value with a Regular Expression.",
      "status": "prod-ready",
      "title": "Regex Parser",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "remove_fields": {
      "beta": false,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"remove_fields\" # required\n  inputs = [\"in\"] # required\n  fields = [\"field1\", \"field2\", \"parent.child\"] # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "shape",
      "id": "remove_fields_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "remove_fields",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to remove one or more log fields.",
      "status": "prod-ready",
      "title": "Remove Fields",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "remove_tags": {
      "beta": false,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"remove_tags\" # required\n  inputs = [\"in\"] # required\n  tags = [\"tag1\", \"tag2\"] # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "metric"
      ],
      "features": [

      ],
      "function_category": "shape",
      "id": "remove_tags_transform",
      "inpuut_types": [
        "metric"
      ],
      "logo_path": null,
      "name": "remove_tags",
      "operating_systems": [

      ],
      "output_types": [
        "metric"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts metric events and allows you to remove one or more metric tags.",
      "status": "prod-ready",
      "title": "Remove Tags",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "rename_fields": {
      "beta": false,
      "config_examples": {
        "toml": "[transforms.out]\n  # General\n  type = \"rename_fields\" # required\n  inputs = [\"in\"] # required\n\n  # Fields\n  fields.old_field_name = \"new_field_name\" # example\n  fields.parent.old_child_name = \"parent.new_child_name\" # example"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "shape",
      "id": "rename_fields_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "rename_fields",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to rename one or more log fields.",
      "status": "prod-ready",
      "title": "Rename Fields",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "sampler": {
      "beta": true,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"sampler\" # required\n  inputs = [\"in\"] # required\n  rate = 10 # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "filter",
      "id": "sampler_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "sampler",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to sample events with a configurable rate.",
      "status": "beta",
      "title": "Sampler",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "split": {
      "beta": false,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"split\" # required\n  inputs = [\"in\"] # required\n  field_names = [\"timestamp\", \"level\", \"message\", \"parent.child\"] # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "parse",
      "id": "split_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "split",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to split a field's value on a _literal_ separator and zip the tokens into ordered field names.",
      "status": "prod-ready",
      "title": "Split",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "swimlanes": {
      "beta": true,
      "config_examples": {
        "toml": "[transforms.out]\n  # General\n  type = \"swimlanes\" # required\n  inputs = [\"in\"] # required\n\n  # Lanes\n  [transforms.out.lanes.`[swimlane-id]`]"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "route",
      "id": "swimlanes_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "swimlanes",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to route events across parallel streams using logical filters.",
      "status": "beta",
      "title": "Swimlanes",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "tag_cardinality_limit": {
      "beta": true,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"tag_cardinality_limit\" # required\n  inputs = [\"in\"] # required\n  mode = \"exact\" # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "metric"
      ],
      "features": [

      ],
      "function_category": "filter",
      "id": "tag_cardinality_limit_transform",
      "inpuut_types": [
        "metric"
      ],
      "logo_path": null,
      "name": "tag_cardinality_limit",
      "operating_systems": [

      ],
      "output_types": [
        "metric"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts metric events and allows you to limit the cardinality of metric tags to prevent downstream disruption of metrics services.",
      "status": "beta",
      "title": "Tag Cardinality Limit",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    },
    "tokenizer": {
      "beta": false,
      "config_examples": {
        "toml": "[transforms.out]\n  type = \"tokenizer\" # required\n  inputs = [\"in\"] # required\n  field_names = [\"timestamp\", \"level\", \"message\", \"parent.child\"] # required"
      },
      "delivery_guarantee": null,
      "description": null,
      "event_types": [
        "log"
      ],
      "features": [

      ],
      "function_category": "parse",
      "id": "tokenizer_transform",
      "inpuut_types": [
        "log"
      ],
      "logo_path": null,
      "name": "tokenizer",
      "operating_systems": [

      ],
      "output_types": [
        "log"
      ],
      "service_providers": [

      ],
      "short_description": "Accepts log events and allows you to tokenize a field's value by splitting on white space, ignoring special wrapping characters, and zip the tokens into ordered field names.",
      "status": "prod-ready",
      "title": "Tokenizer",
      "type": "transform",
      "unsupported_operating_systems": [

      ]
    }
  }
};
